{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline\n",
    "%reload_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_openml('mnist_784',version = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'categories', 'feature_names', 'target_names', 'DESCR', 'details', 'url'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = mnist[\"data\"],mnist[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "70000 images are present in this dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "digit = x.to_numpy()[24]\n",
    "digit_img = digit.reshape(28,28) #since each image has 784 features\n",
    "plt.imshow(digit_img,cmap = \"binary\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets see if the label matches the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[24]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it does"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since the dataset is already split and shuffled\n",
    "no neccessity to use sklearn's train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train , x_test , y_train , y_test = x[:60000],x[60000:],y[:60000],y[60000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# classification using simple perceptron "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "was trying to fit kernelpca...laptop froze :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron(random_state=42)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "perc_clf = Perceptron(tol=1e-3, random_state=42)\n",
    "perc_clf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = perc_clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 943,    0,    5,    4,    4,    3,    4,    1,   13,    3],\n",
       "       [   0, 1108,   16,    0,    0,    1,    3,    1,    5,    1],\n",
       "       [   9,   11,  927,   13,   13,    3,   12,    7,   34,    3],\n",
       "       [   7,    3,   45,  860,    1,   42,    1,   12,   22,   17],\n",
       "       [   1,    1,   10,    1,  859,    1,    3,    1,    9,   96],\n",
       "       [  12,    3,   11,   27,   10,  766,   15,    7,   31,   10],\n",
       "       [  10,    3,   15,    4,    4,   17,  901,    0,    4,    0],\n",
       "       [   0,    9,   17,   11,    4,    1,    1,  904,    3,   78],\n",
       "       [   6,   22,   38,   32,   11,   77,    8,   13,  695,   72],\n",
       "       [   6,    5,    2,   10,   24,   10,    0,   18,    5,  929]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(*args, **kw)>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(cm,cmap = plt.cm.gray)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have 10 rows and 10 columns (0 to 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[60004]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['7', '2', '1', ..., '4', '5', '6']], dtype=object)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_new = y_test.to_numpy()\n",
    "y_test_new.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = y_pred.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8921166666666667"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perc_clf.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "good training overall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# classification using MLPs/NNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(max_iter=300, random_state=42)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp_clf = MLPClassifier(random_state= 42, max_iter=300)\n",
    "mlp_clf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here, the standard sklearns nn does not provide the acc over each epochs \n",
    "hence it is better to use more powerful apis such as keras where we can use gpus to improve the model train time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9908"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_clf.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since we used a powerful approach on such a trivial problem...(basic) the model seems to have learned a lot...almost overfitted\n",
    "as expected as mlp > perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mlp_clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 958,    0,    1,    1,    2,    4,    3,    3,    4,    4],\n",
       "       [   1, 1117,    2,    3,    0,    3,    0,    0,    8,    1],\n",
       "       [   5,    5,  995,    6,    7,    0,    2,    6,    4,    2],\n",
       "       [   1,    0,    5,  970,    1,   13,    0,    4,    2,   14],\n",
       "       [   1,    2,    4,    0,  949,    0,    2,    1,    2,   21],\n",
       "       [   1,    2,    0,   13,    1,  859,    2,    1,    7,    6],\n",
       "       [   5,    4,    0,    0,    3,   11,  929,    0,    3,    3],\n",
       "       [   0,    7,   13,    9,    3,    0,    0,  981,    4,   11],\n",
       "       [   0,    3,    5,   18,    1,   12,    6,    6,  907,   16],\n",
       "       [   3,    5,    1,   12,    8,    3,    0,    2,    4,  971]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(*args, **kw)>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(cm,cmap = plt.cm.gray)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aas we can see we got lot off zeros in the cm\n",
    "which means the model can distinguish the digits with human level accuaracy with very small error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using APIs (keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we can see that one gpu is available and is ready to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = keras.datasets.mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train_all,y_train_all),(x_test,y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one important diff from loading dataset from sklearn and keras is every image is represented as 28 X 28 array rather than 1D array of 784 features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_valid,x_train = x_train_all[:5000] / 255.0, x_train_all[5000:] / 255.0\n",
    "y_valid , y_train = y_train_all[:5000],y_train_all[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 3, 4, ..., 5, 6, 8], dtype=uint8)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape = [28,28]))\n",
    "model.add(keras.layers.Dense((256),activation = \"relu\")) #sigmoid can be used for such a simple problem...but rectifier does perform better and is the standard nowadays\n",
    "model.add(keras.layers.Dense((64),activation = \"relu\"))\n",
    "model.add(keras.layers.Dense((12),activation = \"softmax\")) #we use softmax instead of sigmoid as sigmoid is a bianry function(outputs prob) and softmax outputs the prob of being in a multiclass model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 12)                780       \n",
      "=================================================================\n",
      "Total params: 218,188\n",
      "Trainable params: 218,188\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here the first layer has 784 X 256 weights and 256 biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model has a great flexibility to fit the train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "looking at weights and biases assigned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.core.Flatten at 0x209d5e50888>,\n",
       " <keras.layers.core.Dense at 0x209d5e50cc8>,\n",
       " <keras.layers.core.Dense at 0x209d62f0e48>,\n",
       " <keras.layers.core.Dense at 0x209d5a68d88>]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dense_4'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden1 = model.layers[1] #first hidden layer\n",
    "hidden1.name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights , biases = hidden1.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0387727 ,  0.05592507,  0.05546413, ..., -0.01494284,\n",
       "         0.05000637, -0.02783402],\n",
       "       [-0.01343423,  0.06128719,  0.02866071, ..., -0.07392792,\n",
       "         0.05432893,  0.00429253],\n",
       "       [ 0.01884107, -0.04745711, -0.01686123, ..., -0.0322311 ,\n",
       "         0.06832585, -0.06132018],\n",
       "       ...,\n",
       "       [-0.04200383, -0.04912547, -0.02285286, ..., -0.07066347,\n",
       "         0.02907892,  0.02741208],\n",
       "       [ 0.0686208 ,  0.05221045,  0.00573625, ...,  0.04796688,\n",
       "         0.00362878,  0.01557481],\n",
       "       [-0.06977391,  0.01582229, -0.03272334, ..., -0.01168146,\n",
       "        -0.07508255,  0.07446881]], dtype=float32)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0.], dtype=float32)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can use bias_initializer to tweak our model...but not necesasry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"]) #here we assign the optimizer function,metric function and the cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1719/1719 [==============================] - 61s 3ms/step - loss: 0.2263 - accuracy: 0.9314 - val_loss: 0.1240 - val_accuracy: 0.9644\n",
      "Epoch 2/25\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0927 - accuracy: 0.9717 - val_loss: 0.0824 - val_accuracy: 0.9764\n",
      "Epoch 3/25\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0626 - accuracy: 0.9800 - val_loss: 0.0794 - val_accuracy: 0.9760\n",
      "Epoch 4/25\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0461 - accuracy: 0.9853 - val_loss: 0.1011 - val_accuracy: 0.9722\n",
      "Epoch 5/25\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0368 - accuracy: 0.9875 - val_loss: 0.0850 - val_accuracy: 0.9762\n",
      "Epoch 6/25\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0277 - accuracy: 0.9908 - val_loss: 0.0734 - val_accuracy: 0.9804\n",
      "Epoch 7/25\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0236 - accuracy: 0.9924 - val_loss: 0.0880 - val_accuracy: 0.9768\n",
      "Epoch 8/25\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0197 - accuracy: 0.9933 - val_loss: 0.0731 - val_accuracy: 0.9822\n",
      "Epoch 9/25\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.0166 - accuracy: 0.9949 - val_loss: 0.0897 - val_accuracy: 0.9776\n",
      "Epoch 10/25\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0179 - accuracy: 0.9940 - val_loss: 0.0838 - val_accuracy: 0.9816\n",
      "Epoch 11/25\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0114 - accuracy: 0.9963 - val_loss: 0.1074 - val_accuracy: 0.9794\n",
      "Epoch 12/25\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0147 - accuracy: 0.9952 - val_loss: 0.0854 - val_accuracy: 0.9802\n",
      "Epoch 13/25\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0125 - accuracy: 0.9958 - val_loss: 0.1022 - val_accuracy: 0.9780\n",
      "Epoch 14/25\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0121 - accuracy: 0.9962 - val_loss: 0.1240 - val_accuracy: 0.9762\n",
      "Epoch 15/25\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0109 - accuracy: 0.9966 - val_loss: 0.1274 - val_accuracy: 0.9754\n",
      "Epoch 16/25\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0108 - accuracy: 0.9964 - val_loss: 0.1019 - val_accuracy: 0.9802\n",
      "Epoch 17/25\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0115 - accuracy: 0.9966 - val_loss: 0.1133 - val_accuracy: 0.9792\n",
      "Epoch 18/25\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0097 - accuracy: 0.9971 - val_loss: 0.1221 - val_accuracy: 0.9762\n",
      "Epoch 19/25\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0085 - accuracy: 0.9976 - val_loss: 0.1150 - val_accuracy: 0.9794\n",
      "Epoch 20/25\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0102 - accuracy: 0.9969 - val_loss: 0.1245 - val_accuracy: 0.9796\n",
      "Epoch 21/25\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0084 - accuracy: 0.9971 - val_loss: 0.1238 - val_accuracy: 0.9776\n",
      "Epoch 22/25\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0076 - accuracy: 0.9975 - val_loss: 0.1409 - val_accuracy: 0.9782\n",
      "Epoch 23/25\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0086 - accuracy: 0.9974 - val_loss: 0.1067 - val_accuracy: 0.9828\n",
      "Epoch 24/25\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0092 - accuracy: 0.9974 - val_loss: 0.1405 - val_accuracy: 0.9790\n",
      "Epoch 25/25\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0067 - accuracy: 0.9980 - val_loss: 0.1264 - val_accuracy: 0.9796\n"
     ]
    }
   ],
   "source": [
    "fin = model.fit(x_train,y_train,epochs = 25,validation_data=(x_valid,y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here val accuracy is below 99 hence not overfitted yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as we can see...the NN model using the keras api performs way better than the sklearn NN as here we can increase the hidden layers and number of neurons in it accoirdingly\n",
    "we get freedom of parameter and model tweaking as well as gpu usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we feel like the model is overfitted after a certain epoch...we can stop the model using keras provided functions and also can drop certain layers and neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "saing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"lab11.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the hdf5 format saves the entire architecture of our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can use callbacks and early stopping as said above to get a perfect desired model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using tf for Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = \"logs/scalars/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.0100 - accuracy: 0.9976 - val_loss: 0.1794 - val_accuracy: 0.9806\n",
      "Epoch 2/7\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0067 - accuracy: 0.9984 - val_loss: 0.1735 - val_accuracy: 0.9830\n",
      "Epoch 3/7\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0065 - accuracy: 0.9984 - val_loss: 0.1839 - val_accuracy: 0.9814\n",
      "Epoch 4/7\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0061 - accuracy: 0.9985 - val_loss: 0.1759 - val_accuracy: 0.9818\n",
      "Epoch 5/7\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0033 - accuracy: 0.9992 - val_loss: 0.1906 - val_accuracy: 0.9794\n",
      "Epoch 6/7\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0076 - accuracy: 0.9981 - val_loss: 0.1900 - val_accuracy: 0.9822\n",
      "Epoch 7/7\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0049 - accuracy: 0.9986 - val_loss: 0.1598 - val_accuracy: 0.9824\n"
     ]
    }
   ],
   "source": [
    "fin = model.fit(x_train,y_train,epochs = 7,validation_data=(x_valid,y_valid), callbacks=[tensorboard_callback]) #lets reduce the no. of epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-f450ddbdd82e0c74\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-f450ddbdd82e0c74\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
